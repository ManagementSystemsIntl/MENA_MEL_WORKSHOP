---
title: "Bayesian analysis"
date: "Latest version: `r Sys.Date()`"
format: 
  revealjs:
    #logo: "USAID logo.png"
    slide-number: true
    slide-level: 3
    transition: slide
    #transition-speed: fast
    background-transition: fade
    incremental: true
    footer: "USAID MENA Advanced MEL Workshop"
    theme: [default, usaid notepad.scss]
    title-slide-attributes: 
      data-background-image: "USAID logo.png"
      data-background-position: top left
      data-background-size: 25%
editor: visual
chalkboard: true
---


```{r setup, include=FALSE, message=F, warning=F}

knitr::opts_chunk$set(echo=F, message=F, warning=F, fig.height=10, fig.width=6)

library(here)
library(knitr)

source("../../../Methods Corner/misc/prep.r")

```

### Welcome!

-   Who we are
-   What we do
-   How we hope to help you

### Benchmarks for success

By the end of this session, participants will be able to:

- Understand how to derive Bayes' Rule from the laws of probability

- Understand how to interpret Bayes' Rule in the context of a data analysis

- Understand how thinking like a Bayesian follows the scientific method and should be a normal part of our thinking about the world

### Benchmarks for success

*Bonus content:*

-   Setting Bayesian priors as qualitative research 

-   Naive Bayes'

-   Expectation Maximization 

### [Level Set]{style="color:white;"} {background-color="#002F6C"}

[Deriving Bayes' Rule](https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/)


### The Frequentist and the Bayesian

Frequentist hypothesis test: $P(data>data_{observed}|H_0)$

The likelihood: $P(data|model)$

The posterior probability: $P(model|data)$

Bayesian analysis requires us to update our beliefs in light of new data 

### Bayes' Rule

Consider two overlapping events A and B occurring within a universe U.

```{r}
include_graphics("venn-last.png")
```

$P(A)=\frac{A}{U}$

$P(AB)=\frac{AB}{U}$

### How much of A is in B? 

```{r}
include_graphics("venn-last.png")
```

$P(A|B)=\frac{P(AB)}{P(B)}$  

$P(AB)=P(A|B)P(B)$

### How much of B is in A? 

```{r}
include_graphics("venn-last.png")
```

$P(B|A)=\frac{P(AB)}{P(A)}$  

$P(AB)=P(B|A)P(A)$

### Putting the two together

We can put the two identities together and solve for $A|B$:  

$P(A|B)P(B)=P(B|A)P(A)$  


$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$  

And that's it.  That's Bayes' Rule.

### Bayes' Rule as an analytical tool

We just used the laws of probability to derive Bayes' Rule: 

$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$  


Now let's use this in the context of a data analysis

$P(model|data)=\frac{P(data|model)P(model)}{P(data)}$

### Using Bayes' Rule

$$P(model|data)=\frac{P(data|model)P(model)}{P(data)}$$

$P(model|data)$: probability of a hypothesis given data  

$P(data|model)$: the likelihood of our data for each hypothesis  

$P(model)$: the prior probability of the model, before data

### Bayesian inference - example problem

- 40 subjects, half randomly assigned a treatment

- The treatment is expected to reduce the probability of an event

- What is the probability *p* that an observed event occurred within the treatment group?

### Setting up our hypotheses

$H_0: p=50\%$ No treatment effect

$H_1: p \lt 50\%$ Treatment effect

- 20 events - 4 events in the treatment group and 16 events in the control group

- How likely are these four events to have occurred within the treatment group?

### Setting up the Bayesian engine

1. Set a range of plausible values (the model space)
2. Calculate the likelihood of the data for each plausible value
3. Set the prior probability of each plausible value
4. Multiply the likelihood by the prior (numerator)
5. Divide by the denominator to get the posterior probability

### Results of the Bayesian engine

```{r}
p <- seq(from=.1,
         to=.9,
         by=.1)

likelihood <- dbinom(x=4,
                     size=20,
                     prob=p)

prior <- c(rep(.06,4), .52, rep(.06,4))

numerator <- likelihood*prior

denominator <- sum(numerator)

posterior <- numerator / denominator

out <- data.frame(hypothesis=p, likelihood, prior, numerator, posterior) %>%
  round(3)

flextable(out) %>%
  bg(i = ~posterior > .3,
     bg="yellow", part="body") %>%
  set_formatter(hypothesis=percent,
                prior=percent,
                posterior = percent) 

```

A treatment effect of 20 percent is most likely

But notice that we get back an entire distribution, not just a point estimate

### From prior to posterior 

```{r fig.height=4, fig.width=7}
library(ggchicklet)

ot <- out %>%
  select(hypothesis, prior, posterior) %>%
  pivot_longer(cols=2:3,
               names_to="measure",
               values_to="value")

ggplot(ot, aes(x=hypothesis, y=value, group=measure, color=measure, fill=measure)) +
  geom_chicklet(position=position_dodge(),
           width=.05,
           alpha=.8) +
  scale_color_manual(values=c(usaid_red, usaid_blue)) +
  scale_fill_manual(values=c(usaid_red, usaid_blue)) +
  scale_x_continuous(breaks=seq(.1,.9,.1),
                     labels=percent_format(accuracy=1)) + 
  scale_y_continuous(limits=c(0,.55),
                     breaks=seq(0,.55,.05),
                     labels=percent_format(accuracy=1)) +
  labs(x="Candidate model value",
       y="Probability",
       title="Probability of candidate model values<br>
       <span style='color:#BA0C2F;'>Posterior distribution</span>
       <span style='color:#002F6C;'>Prior distribution</span>") +
  theme(axis.title.y=element_text(angle=0, vjust=.55),
        plot.title=element_markdown(),
        legend.position="none")
```

### Bayesian analysis as science

- Recall what we learned as kids about the scientific method:  
      - Observe a state of the world  
      - Develop a hypothesis about how the world works  
      - Test your hypothesis with new data  
      - Update your beliefs and repeat  
  
### Think like a Bayesian

- Using Bayes' Rule to conduct inference follows the scientific method! 

- Let's think like a Bayesian

- Stay tuned for Bayes' Rule used in machine learning

Thank you!

### Bonus content

- Setting Bayesian priors as qualitative research

- Naive Bayes Classifier

- Expectation Maximization (EM) algorithm










